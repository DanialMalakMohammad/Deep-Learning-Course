{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "-AWEKWmWQ5JM",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# Corpus Cell\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "window_length = 5\n",
    "embedding_dimension = 25\n",
    "\n",
    "\n",
    "def tokenize(data):\n",
    "    data = data.replace('\\n', ' , ')\n",
    "    data = data.replace('\\t', ' ')\n",
    "    data = data.replace('\\r', ' ')\n",
    "    data = data.replace(',', '.')\n",
    "    data = data.lower()\n",
    "    data = re.sub(' +', ' ', data)\n",
    "#     data = data.split()\n",
    "    return data\n",
    "\n",
    "data = open('ferdosi.txt', 'r', encoding='utf-8').read()\n",
    "\n",
    "data = tokenize(data)\n",
    "corpus_raw = data\n",
    "corpus_raw = corpus_raw.lower()\n",
    "\n",
    "words = []\n",
    "for word in corpus_raw.split():\n",
    "    if word != '.': \n",
    "        words.append(word)\n",
    "\n",
    "words = set(words) \n",
    "word_to_index = {}\n",
    "index_to_word = {}\n",
    "word_count = len(words) \n",
    "words_list = list(words)\n",
    "  \n",
    "for i,word in enumerate(words):\n",
    "    word_to_index[word] = i\n",
    "    index_to_word[i] = word\n",
    "\n",
    "    \n",
    "\n",
    "raw_sentences = corpus_raw.split('.')\n",
    "sentences = []\n",
    "for sentence in raw_sentences:\n",
    "    sentences.append(sentence.split())\n",
    "\n",
    "    \n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "unigram = np.zeros(word_count)\n",
    "\n",
    "for mesra in sentences:\n",
    "  for word in mesra:\n",
    "    unigram[word_to_index[word]] += 1\n",
    "\n",
    "unigram=unigram/unigram.sum()\n",
    "\n",
    "smoothed_unigram=np.power(unigram,3/4)\n",
    "smoothed_unigram=smoothed_unigram/smoothed_unigram.sum()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "co_occurrence_matrix=np.zeros((word_count,word_count))\n",
    "\n",
    "for mesra in sentences:\n",
    "  for target_iterator in range(len(mesra)):\n",
    "    for context_iterator in range(  max(0,target_iterator-window_length), min(len(mesra),target_iterator+window_length+1)   ):\n",
    "      if context_iterator != target_iterator:\n",
    "        co_occurrence_matrix[word_to_index[mesra[target_iterator]],word_to_index[mesra[context_iterator]]]+=1\n",
    "\n",
    "smoothed_negative_samples_unigram =np.zeros((word_count,word_count))\n",
    "smoothed_negative_samples_unigram [np.nonzero(co_occurrence_matrix)]=1\n",
    "smoothed_negative_samples_unigram = 1-smoothed_negative_samples_unigram\n",
    "smoothed_negative_samples_unigram = smoothed_negative_samples_unigram*unigram.reshape(1,-1)\n",
    "smoothed_negative_samples_unigram = np.power(smoothed_negative_samples_unigram,0.75)\n",
    "smoothed_negative_samples_unigram = smoothed_negative_samples_unigram/np.sum(smoothed_negative_samples_unigram,axis=1).reshape(-1,1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "9K-xyXhZdtNv",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "#Model Definition Cell\n",
    "\n",
    "batch_size = 128\n",
    "negative_sample_count = 10\n",
    "learning_rate=0.001\n",
    " \n",
    "\n",
    "W      = tf.Variable(tf.random_normal([word_count, embedding_dimension], stddev=0.00001))\n",
    "W_prim = tf.Variable(tf.random_normal([word_count, embedding_dimension], stddev=0.00001))\n",
    "\n",
    "\n",
    "target           = tf.placeholder(tf.int32, shape=(None,))\n",
    "context          = tf.placeholder(tf.int32, shape=(None,))\n",
    "negative_samples = tf.placeholder(tf.int32, shape=(None,negative_sample_count,))\n",
    "\n",
    "\n",
    "v_target           = tf.gather(W,indices=target,axis=0)\n",
    "u_context          = tf.gather(W_prim,indices=context,axis=0)\n",
    "u_negative_samples = tf.gather_nd(W_prim, indices=tf.reshape(negative_samples,shape=(-1,negative_sample_count,1)))\n",
    "\n",
    "\n",
    "positive_score=tf.log(tf.sigmoid(tf.reduce_sum(tf.multiply(u_context,v_target),axis=1)))\n",
    "negative_score=tf.reduce_sum(tf.log(tf.sigmoid(-tf.reduce_sum(tf.multiply(u_negative_samples,tf.reshape(v_target,[-1,1,embedding_dimension])),axis=2))),axis=1)\n",
    "\n",
    "loss = -tf.reduce_mean(positive_score+negative_score,axis=0)\n",
    "\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "kR8Sw43DQ7jj",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "#Training Cell\n",
    "#Once you run this cell, your model gets trained on one epoch data i.e. a pass on corpus\n",
    "\n",
    "target_batch =np.zeros((batch_size,))\n",
    "context_batch=np.zeros((batch_size,))\n",
    "negative_samples_batch=np.zeros((batch_size,negative_sample_count,))\n",
    "\n",
    "sample_counter=0\n",
    "batch_counter=0\n",
    "for mesra in sentences:\n",
    "  for target_iterator in range(len(mesra)):\n",
    "    for context_iterator in range(  max(0,target_iterator-window_length), min(len(mesra),target_iterator+window_length+1)   ):\n",
    "      if context_iterator != target_iterator:\n",
    "        target_batch[sample_counter]           = word_to_index[mesra[target_iterator ]]\n",
    "        context_batch[sample_counter]          = word_to_index[mesra[context_iterator]]\n",
    "        negative_samples_batch[sample_counter] = np.random.choice(np.arange(word_count), negative_sample_count, p=smoothed_negative_samples_unigram[word_to_index[mesra[target_iterator]]])\n",
    "        sample_counter+=1\n",
    "        if sample_counter==batch_size-1:\n",
    "          sess.run(train_step, feed_dict={target: target_batch, context: context_batch, negative_samples: negative_samples_batch})\n",
    "          if batch_counter%200==0:\n",
    "            print(\"batch : \", batch_counter,\"   ,   loss : \",sess.run(loss, feed_dict={target: target_batch, context: context_batch, negative_samples: negative_samples_batch}))\n",
    "          sample_counter=0\n",
    "          batch_counter+=1\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "V_ZFEmo1lnWF",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# Save parameters Cell\n",
    "np.savetxt('W_saved.txt',sess.run(W))\n",
    "np.savetxt('W_prim_saved.txt',sess.run(W_prim))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "B1YdO9RMo0I6",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# Load Parameters Cell\n",
    "\n",
    "W_loaded=np.loadtxt('W_saved.txt', dtype=np.double)\n",
    "W_prim_loaded=np.loadtxt('W_prim_saved.txt', dtype=np.double)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "Ib1pOZGnpRDS",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51.0
    },
    "outputId": "f62dcb9a-661c-47ce-d53a-e3c72d9bdab1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.90475746 0.89054388 0.88912665 0.88815269 0.86909583 0.85987825]\n",
      "['دانا', 'خوی', 'جهاندار', 'هوش', 'دستور', 'رای']\n"
     ]
    }
   ],
   "source": [
    "# Test cell\n",
    "\n",
    "target_word='خردمند'\n",
    "embedding=W_loaded\n",
    "\n",
    "\n",
    "\n",
    "target_vector=embedding[word_to_index[target_word]]\n",
    "\n",
    "normalized_embedding= embedding / np.sqrt(np.power(embedding,2).sum(axis=1).reshape(-1,1))\n",
    "target_vector_normalized = target_vector / np.sqrt( target_vector.reshape(1,-1)@target_vector.reshape(-1,1) )\n",
    "\n",
    "context_scores=(normalized_embedding * target_vector_normalized.reshape(1,-1)).sum(axis=1)\n",
    "\n",
    "best_neighbor_indices=np.flip(np.argsort(context_scores)[-7:-1])\n",
    "\n",
    "print(context_scores[best_neighbor_indices])\n",
    "best_neighbor_words=[]\n",
    "for t in best_neighbor_indices:\n",
    "  best_neighbor_words.append(index_to_word[t])\n",
    "print(best_neighbor_words)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Word2vec_last_version",
   "version": "0.3.2",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
