{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_Project_v10.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjhxQgSRXxOe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#### Data Preparation Cell\n",
        "\n",
        "import numpy as np\n",
        "import torch.nn\n",
        "import torch.nn.functional \n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "train_x      = np.load(\"/content/drive/My Drive/Deep_project/Train.npy\"     )\n",
        "test_x       = np.load(\"/content/drive/My Drive/Deep_project/Test.npy\"      )\n",
        "validation_x = np.load(\"/content/drive/My Drive/Deep_project/Validation.npy\")\n",
        "\n",
        "\n",
        "with open('/content/drive/My Drive/Deep_project/formulas/train_formulas.txt', 'r') as f:\n",
        "    train_y = f.readlines()\n",
        "    \n",
        "with open('/content/drive/My Drive/Deep_project/formulas/validation_formulas.txt', 'r') as f:\n",
        "    validation_y = f.readlines()\n",
        "\n",
        "\n",
        "###############################################################\n",
        "\n",
        "token_dict={}\n",
        "token_to_index={}\n",
        "index_to_token={}\n",
        "\n",
        "\n",
        "for sentence in train_y:\n",
        "  for token in sentence.split():\n",
        "    token_dict[token]=1\n",
        "  \n",
        "token_list=token_dict.keys()\n",
        "\n",
        "\n",
        "for index,token in enumerate(token_list,0):\n",
        "  token_to_index[token]=index\n",
        "  index_to_token[index]=token\n",
        "  \n",
        "  \n",
        "bos_index=len(token_list)\n",
        "eos_index=len(token_list)+1\n",
        "index_to_token[len(token_list)]='bos'\n",
        "index_to_token[len(token_list)+1]='eos'\n",
        "\n",
        "\n",
        "##############################################################\n",
        "\n",
        "maximum_length_formula=0\n",
        "maxim = 0 \n",
        "\n",
        "for i in range(len(train_y)):\n",
        "  if len(train_y[i].split())>maximum_length_formula:\n",
        "    maximum_length_formula=len(train_y[i].split())\n",
        "    \n",
        "for i in range(len(validation_y)):\n",
        "  if len(validation_y[i].split())>maximum_length_formula:\n",
        "    maximum_length_formula=len(validation_y[i].split())\n",
        "\n",
        "    \n",
        "#########################################################    \n",
        "    \n",
        "train_y_processed = np.ones((len(train_y),maximum_length_formula+3))*eos_index\n",
        "\n",
        "for i in range(len(train_y)):\n",
        "  \n",
        "  sentence = train_y[i].split()\n",
        "  train_y_processed[i,0]=bos_index\n",
        "  train_y_processed[i,-1]=len(sentence)+2\n",
        "  \n",
        "  for j in range(len(sentence)):\n",
        "    train_y_processed[i,j+1]=token_to_index[sentence[j]]\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6vUkmVmxOE_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Model Definition Cell\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class reshape_module(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, *args):\n",
        "        super(reshape_module, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x.transpose(dim0=1, dim1=3).contiguous().view(x.shape[0], 50, 512 * 7)\n",
        "\n",
        "\n",
        "class Decoder(torch.nn.Module):\n",
        "    def __init__(self, embedding_dim, decoder_hidden_dim, encoder_hidden_dim, attention_first_layer_dim, O_dim,\n",
        "                 token_num, max_length):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.decoder_hidden_dim = decoder_hidden_dim\n",
        "        self.attention_first_layer_dim = attention_first_layer_dim\n",
        "        self.O_dim = O_dim\n",
        "        self.max_length = max_length\n",
        "        self.token_num = token_num\n",
        "        self.encoder_hidden_dim = encoder_hidden_dim\n",
        "\n",
        "        self.embedding = torch.nn.Embedding(token_num, embedding_dim)\n",
        "\n",
        "        self.W_h = torch.nn.Linear(decoder_hidden_dim, attention_first_layer_dim)\n",
        "        self.W_v = torch.nn.Linear(encoder_hidden_dim, attention_first_layer_dim)\n",
        "        self.beta = torch.nn.Linear(attention_first_layer_dim, 1)\n",
        "        self.W_c_h = torch.nn.Linear(decoder_hidden_dim, O_dim)\n",
        "        self.W_c_c = torch.nn.Linear(encoder_hidden_dim, O_dim)\n",
        "        self.W_out = torch.nn.Linear(O_dim, token_num)\n",
        "\n",
        "        self.lstm = torch.nn.LSTM(input_size=embedding_dim + O_dim, hidden_size=512, num_layers=1, bias=True,\n",
        "                                  batch_first=False, bidirectional=False)\n",
        "\n",
        "    def forward(self, y_before, o_before, c_memory_before, h_before, encoder_hiddens):\n",
        "        \n",
        "        input = torch.cat((self.embedding(y_before), o_before), dim=1).unsqueeze(dim=0)\n",
        "\n",
        "        _, (h_t, c_t) = self.lstm(input, (h_before, c_memory_before))\n",
        "\n",
        "        attention_first_layer_output = self.W_v(encoder_hiddens) + torch.unsqueeze(self.W_h(h_t.squeeze(dim=0)), 1)\n",
        "        attention_e = self.beta(torch.nn.functional.tanh(attention_first_layer_output)).squeeze(dim=2)\n",
        "        attention_alpha = torch.nn.functional.softmax(attention_e, dim=1)\n",
        "\n",
        "        context = torch.sum(attention_alpha.unsqueeze(2) * encoder_hiddens, dim=1)\n",
        "\n",
        "        o_t = self.W_c_c(context) + self.W_c_h(h_t.squeeze(dim=0))\n",
        "        p_t = torch.nn.functional.log_softmax(self.W_out(o_t), dim=1)\n",
        "\n",
        "        return p_t, o_t, c_t, h_t\n",
        "\n",
        "\n",
        "class Model():\n",
        "\n",
        "    def __init__(self, *args):\n",
        "\n",
        "        self.init_conv_encoder()\n",
        "\n",
        "        self.decoder = Decoder(embedding_dim=80, decoder_hidden_dim=512, encoder_hidden_dim=512,\n",
        "                               attention_first_layer_dim=200, O_dim=512, token_num=564, max_length=50).cuda()\n",
        "\n",
        "        self.optimizer = torch.optim.Adam(params=list(self.conv_encoder.parameters()) + list(self.decoder.parameters()), lr=0.001)\n",
        "        \n",
        "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=1, gamma=0.9)\n",
        "    \n",
        "    def init_conv_encoder(self):\n",
        "        self.conv_encoder = torch.nn.Sequential(\n",
        "\n",
        "            #####################################################################\n",
        "\n",
        "            torch.nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, stride=1,\n",
        "                            padding=1, dilation=1, groups=1,\n",
        "                            bias=True, padding_mode='zeros'),\n",
        "\n",
        "            torch.nn.ReLU(inplace=True),\n",
        "\n",
        "            torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
        "\n",
        "            #####################################################################\n",
        "\n",
        "            torch.nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1,\n",
        "                            padding=1, dilation=1, groups=1,\n",
        "                            bias=True, padding_mode='zeros'),\n",
        "\n",
        "            torch.nn.ReLU(inplace=True),\n",
        "\n",
        "            torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
        "\n",
        "            ######################################################################\n",
        "\n",
        "            torch.nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1,\n",
        "                            padding=1, dilation=1, groups=1,\n",
        "                            bias=True, padding_mode='zeros'),\n",
        "\n",
        "            torch.nn.BatchNorm2d(256),\n",
        "            torch.nn.ReLU(inplace=True),\n",
        "\n",
        "            ######################################################################\n",
        "\n",
        "            torch.nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1,\n",
        "                            padding=1, dilation=1, groups=1,\n",
        "                            bias=True, padding_mode='zeros'),\n",
        "\n",
        "            torch.nn.ReLU(inplace=True),\n",
        "\n",
        "            torch.nn.MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0),\n",
        "\n",
        "            ######################################################################\n",
        "\n",
        "            torch.nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1,\n",
        "                            padding=1, dilation=1, groups=1,\n",
        "                            bias=True, padding_mode='zeros'),\n",
        "\n",
        "            torch.nn.BatchNorm2d(512),\n",
        "            torch.nn.ReLU(inplace=True),\n",
        "\n",
        "            ######################################################################\n",
        "\n",
        "            torch.nn.MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0),\n",
        "\n",
        "            torch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1,\n",
        "                            padding=1, dilation=1, groups=1,\n",
        "                            bias=True, padding_mode='zeros'),\n",
        "\n",
        "            torch.nn.BatchNorm2d(512),\n",
        "            torch.nn.ReLU(inplace=True),\n",
        "\n",
        "            reshape_module(),\n",
        "\n",
        "            torch.nn.LSTM(input_size=7 * 512, hidden_size=256, num_layers=1, bias=True, batch_first=True,\n",
        "                          bidirectional=True)\n",
        "\n",
        "        )\n",
        "        self.conv_encoder.cuda()\n",
        "\n",
        "\n",
        "    def save_model(self,path,name_string):\n",
        "\n",
        "        dest_path = path+\"/\"+name_string\n",
        "        os.mkdir(dest_path)\n",
        "\n",
        "        torch.save(self.conv_encoder.state_dict(),dest_path+\"/conv_encoder.pt\")\n",
        "        torch.save(self.decoder.state_dict(),dest_path+\"/decoder.pt\")\n",
        "\n",
        "\n",
        "    def load_model(self,path,name_string):\n",
        "\n",
        "        dest_path = path + \"/\" + name_string\n",
        "\n",
        "        self.conv_encoder.load_state_dict(torch.load(dest_path+\"/conv_encoder.pt\"))\n",
        "        self.decoder.load_state_dict(torch.load(dest_path+\"/decoder.pt\"))\n",
        "\n",
        "    def eval_mode(self):\n",
        "        self.conv_encoder.eval()\n",
        "        self.decoder.eval()\n",
        "\n",
        "    def train_mode(self):\n",
        "        self.conv_encoder.train()\n",
        "        self.decoder.train()\n",
        "\n",
        "\n",
        "\n",
        "    def teacher_force_train(self, image, y):\n",
        "        encoder_hiddens, _ = self.conv_encoder(image)\n",
        "\n",
        "        o_tmp = torch.Tensor(np.zeros((image.shape[0], self.decoder.O_dim))).cuda()\n",
        "        c_memory_tmp = torch.Tensor(np.zeros((1, image.shape[0], self.decoder.decoder_hidden_dim))).cuda()\n",
        "        h_tmp = encoder_hiddens[:, -1, :].unsqueeze(0)\n",
        "        encoder_hiddens_tmp = encoder_hiddens\n",
        "        loss = torch.Tensor(np.zeros((1))).cuda()\n",
        "\n",
        "        for time_step in range(y.shape[1] - 1):\n",
        "            # (self, y_before,o_before,c_memory_before, h_before, encoder_hiddens)\n",
        "            # p_t,o_t,c_t,h_t\n",
        "\n",
        "            p_t, o_tmp, c_memory_tmp, h_tmp = self.decoder(y_before=y[:, time_step], o_before=o_tmp,\n",
        "                                                           c_memory_before=c_memory_tmp, h_before=h_tmp,\n",
        "                                                           encoder_hiddens=encoder_hiddens_tmp)\n",
        "\n",
        "            loss_tmp = torch.nn.functional.nll_loss(input=p_t, target=y[:, time_step + 1], reduction='mean')\n",
        "            loss += loss_tmp\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "    \n",
        "\n",
        "    def test(self,image):\n",
        "\n",
        "        encoder_hiddens, _ = self.conv_encoder(image)\n",
        "\n",
        "        o_tmp = torch.Tensor(np.zeros((1, self.decoder.O_dim))).cuda()\n",
        "        c_memory_tmp = torch.Tensor(np.zeros((1, 1, self.decoder.decoder_hidden_dim))).cuda()\n",
        "        h_tmp = encoder_hiddens[:, -1, :].unsqueeze(0)\n",
        "        encoder_hiddens_tmp = encoder_hiddens\n",
        "        y_tmp = torch.LongTensor(np.ones(1) * bos_index).cuda()\n",
        "\n",
        "        result_list = []\n",
        "        result_string = \"\"\n",
        "\n",
        "        counter = 0\n",
        "\n",
        "        while y_tmp != eos_index and len(result_list) < 180:\n",
        "            counter += 1\n",
        "            p_t, o_tmp, c_memory_tmp, h_tmp = self.decoder(y_before=y_tmp, o_before=o_tmp, c_memory_before=c_memory_tmp,\n",
        "                                                           h_before=h_tmp, encoder_hiddens=encoder_hiddens_tmp)\n",
        "\n",
        "            y_tmp = p_t.argmax(dim=1)\n",
        "            result_list.append(y_tmp)\n",
        "            result_string += (index_to_token[y_tmp.item()] + \" \")\n",
        "\n",
        "        return result_string[0:-5]\n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    def make_formula_file(self,x,file_name):\n",
        "\n",
        "\n",
        "      string_result=\"\"\n",
        "\n",
        "      for i in range(x.shape[0]):\n",
        "        formula_string = self.test(torch.Tensor(x[i]/128-1).unsqueeze(0).unsqueeze(0).cuda())                                          \n",
        "        string_result+=(formula_string+\"\\n\")\n",
        "\n",
        "        if i%500==0:\n",
        "          print (i)\n",
        "\n",
        "      open(file_name,'w').write(string_result)\n",
        "      \n",
        "\n",
        " \n",
        "    \n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cGi161ySvUQ",
        "colab_type": "code",
        "outputId": "9688f07f-2b1b-401a-cd09-defca945c440",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "## Training Cell\n",
        "\n",
        "\n",
        "epoch_num = 30\n",
        "batch_size = 128\n",
        "\n",
        "#################\n",
        "\n",
        "#os.mkdir(\"/content/drive/My Drive/Deep_project/version10_models\")\n",
        "#my_model = Model()\n",
        "#my_model.load_model(\"/content/drive/My Drive/Deep_project/version3_models\",\"epoch_25\")\n",
        "#my_model.optimizer = torch.optim.Adam(params=list(my_model.conv_encoder.parameters()) + list(my_model.decoder.parameters()), lr=4e-5)     \n",
        "#my_model.scheduler = torch.optim.lr_scheduler.StepLR(my_model.optimizer, step_size=1, gamma=0.9)\n",
        "\n",
        "#################\n",
        "\n",
        "\n",
        "for epoch_counter in range(epoch_num):\n",
        "  \n",
        "  print(\"Epoch number : \",epoch_counter)\n",
        "  \n",
        "  permutation = np.arange(train_x.shape[0])\n",
        "  np.random.shuffle(permutation)\n",
        "    \n",
        "  for batch_counter in range(int(train_x.shape[0]/batch_size)-2):\n",
        "    \n",
        "    batch_max_label_length = int(train_y_processed[permutation[batch_size*batch_counter:batch_size*(batch_counter+1)]][:,-1].max())\n",
        "                                    \n",
        "    batch_x = torch.Tensor(              train_x[permutation[batch_size*batch_counter:batch_size*(batch_counter+1)]]/128 -1).unsqueeze(1).cuda()\n",
        "    batch_y = torch.LongTensor(train_y_processed[permutation[batch_size*batch_counter:batch_size*(batch_counter+1)]][:,0:batch_max_label_length+1]).cuda()\n",
        "    \n",
        "    loss = my_model.teacher_force_train(batch_x,batch_y)\n",
        "    \n",
        "    if batch_counter%50 == 0:\n",
        "      print(\"batch_number= \",batch_counter,\" , loss= \",loss)\n",
        "      \n",
        "  \n",
        "  my_model.save_model(\"/content/drive/My Drive/Deep_project/version10_models\",\"epoch_\"+str(epoch_counter))\n",
        "  my_model.scheduler.step()\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch number :  0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1374: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "batch_number=  0  , loss=  717.60498046875\n",
            "batch_number=  50  , loss=  165.49234008789062\n",
            "batch_number=  100  , loss=  148.3910369873047\n",
            "batch_number=  150  , loss=  144.10113525390625\n",
            "batch_number=  200  , loss=  134.6372528076172\n",
            "batch_number=  250  , loss=  128.30055236816406\n",
            "batch_number=  300  , loss=  123.25305938720703\n",
            "batch_number=  350  , loss=  115.32899475097656\n",
            "batch_number=  400  , loss=  112.43504333496094\n",
            "batch_number=  450  , loss=  116.22109985351562\n",
            "batch_number=  500  , loss=  107.33399963378906\n",
            "Epoch number :  1\n",
            "batch_number=  0  , loss=  112.60838317871094\n",
            "batch_number=  50  , loss=  107.84037780761719\n",
            "batch_number=  100  , loss=  106.7607192993164\n",
            "batch_number=  150  , loss=  99.85017395019531\n",
            "batch_number=  200  , loss=  97.67574310302734\n",
            "batch_number=  250  , loss=  94.60527038574219\n",
            "batch_number=  300  , loss=  96.83824157714844\n",
            "batch_number=  350  , loss=  84.03682708740234\n",
            "batch_number=  400  , loss=  86.33174133300781\n",
            "batch_number=  450  , loss=  76.6901626586914\n",
            "batch_number=  500  , loss=  72.31044006347656\n",
            "Epoch number :  2\n",
            "batch_number=  0  , loss=  64.66316223144531\n",
            "batch_number=  50  , loss=  55.59710693359375\n",
            "batch_number=  100  , loss=  58.454254150390625\n",
            "batch_number=  150  , loss=  49.90058135986328\n",
            "batch_number=  200  , loss=  37.5278434753418\n",
            "batch_number=  250  , loss=  51.21662902832031\n",
            "batch_number=  300  , loss=  34.006404876708984\n",
            "batch_number=  350  , loss=  39.170989990234375\n",
            "batch_number=  400  , loss=  28.4371337890625\n",
            "batch_number=  450  , loss=  28.9161319732666\n",
            "batch_number=  500  , loss=  29.9942684173584\n",
            "Epoch number :  3\n",
            "batch_number=  0  , loss=  29.70257568359375\n",
            "batch_number=  50  , loss=  27.188276290893555\n",
            "batch_number=  100  , loss=  20.92002296447754\n",
            "batch_number=  150  , loss=  24.880367279052734\n",
            "batch_number=  200  , loss=  25.735986709594727\n",
            "batch_number=  250  , loss=  27.293445587158203\n",
            "batch_number=  300  , loss=  23.273712158203125\n",
            "batch_number=  350  , loss=  20.878711700439453\n",
            "batch_number=  400  , loss=  18.934398651123047\n",
            "batch_number=  450  , loss=  20.423404693603516\n",
            "batch_number=  500  , loss=  17.605384826660156\n",
            "Epoch number :  4\n",
            "batch_number=  0  , loss=  19.94744110107422\n",
            "batch_number=  50  , loss=  21.29888343811035\n",
            "batch_number=  100  , loss=  19.005456924438477\n",
            "batch_number=  150  , loss=  20.926437377929688\n",
            "batch_number=  200  , loss=  18.026763916015625\n",
            "batch_number=  250  , loss=  17.520158767700195\n",
            "batch_number=  300  , loss=  16.156679153442383\n",
            "batch_number=  350  , loss=  14.913236618041992\n",
            "batch_number=  400  , loss=  15.882024765014648\n",
            "batch_number=  450  , loss=  18.008413314819336\n",
            "batch_number=  500  , loss=  19.0576114654541\n",
            "Epoch number :  5\n",
            "batch_number=  0  , loss=  13.301816940307617\n",
            "batch_number=  50  , loss=  14.276676177978516\n",
            "batch_number=  100  , loss=  14.700056076049805\n",
            "batch_number=  150  , loss=  13.177486419677734\n",
            "batch_number=  200  , loss=  14.653356552124023\n",
            "batch_number=  250  , loss=  13.211935043334961\n",
            "batch_number=  300  , loss=  14.185669898986816\n",
            "batch_number=  350  , loss=  16.216617584228516\n",
            "batch_number=  400  , loss=  32.55455017089844\n",
            "batch_number=  450  , loss=  17.615036010742188\n",
            "batch_number=  500  , loss=  16.999366760253906\n",
            "Epoch number :  6\n",
            "batch_number=  0  , loss=  15.234286308288574\n",
            "batch_number=  50  , loss=  14.15478801727295\n",
            "batch_number=  100  , loss=  12.791003227233887\n",
            "batch_number=  150  , loss=  12.372186660766602\n",
            "batch_number=  200  , loss=  11.311227798461914\n",
            "batch_number=  250  , loss=  13.311832427978516\n",
            "batch_number=  300  , loss=  13.18716049194336\n",
            "batch_number=  350  , loss=  12.87522029876709\n",
            "batch_number=  400  , loss=  12.501143455505371\n",
            "batch_number=  450  , loss=  12.375249862670898\n",
            "batch_number=  500  , loss=  12.107250213623047\n",
            "Epoch number :  7\n",
            "batch_number=  0  , loss=  9.367774963378906\n",
            "batch_number=  50  , loss=  9.218368530273438\n",
            "batch_number=  100  , loss=  10.955141067504883\n",
            "batch_number=  150  , loss=  10.707905769348145\n",
            "batch_number=  200  , loss=  9.451955795288086\n",
            "batch_number=  250  , loss=  9.670297622680664\n",
            "batch_number=  300  , loss=  10.109280586242676\n",
            "batch_number=  350  , loss=  12.329304695129395\n",
            "batch_number=  400  , loss=  20.129907608032227\n",
            "batch_number=  450  , loss=  11.554550170898438\n",
            "batch_number=  500  , loss=  10.876262664794922\n",
            "Epoch number :  8\n",
            "batch_number=  0  , loss=  10.593034744262695\n",
            "batch_number=  50  , loss=  8.490978240966797\n",
            "batch_number=  100  , loss=  9.495641708374023\n",
            "batch_number=  150  , loss=  9.1795654296875\n",
            "batch_number=  200  , loss=  9.333174705505371\n",
            "batch_number=  250  , loss=  7.120613098144531\n",
            "batch_number=  300  , loss=  10.40865421295166\n",
            "batch_number=  350  , loss=  8.565099716186523\n",
            "batch_number=  400  , loss=  8.735502243041992\n",
            "batch_number=  450  , loss=  7.399383544921875\n",
            "batch_number=  500  , loss=  10.09323787689209\n",
            "Epoch number :  9\n",
            "batch_number=  0  , loss=  8.882906913757324\n",
            "batch_number=  50  , loss=  6.708815097808838\n",
            "batch_number=  100  , loss=  7.4269514083862305\n",
            "batch_number=  150  , loss=  9.368470191955566\n",
            "batch_number=  200  , loss=  9.006584167480469\n",
            "batch_number=  250  , loss=  7.336279392242432\n",
            "batch_number=  300  , loss=  7.958014011383057\n",
            "batch_number=  350  , loss=  7.260470867156982\n",
            "batch_number=  400  , loss=  9.480306625366211\n",
            "batch_number=  450  , loss=  8.162554740905762\n",
            "batch_number=  500  , loss=  7.245044708251953\n",
            "Epoch number :  10\n",
            "batch_number=  0  , loss=  6.222774028778076\n",
            "batch_number=  50  , loss=  7.291905879974365\n",
            "batch_number=  100  , loss=  6.597949028015137\n",
            "batch_number=  150  , loss=  7.853384017944336\n",
            "batch_number=  200  , loss=  6.3809614181518555\n",
            "batch_number=  250  , loss=  6.866429328918457\n",
            "batch_number=  300  , loss=  10.771617889404297\n",
            "batch_number=  350  , loss=  9.11154556274414\n",
            "batch_number=  400  , loss=  7.638739109039307\n",
            "batch_number=  450  , loss=  7.578649520874023\n",
            "batch_number=  500  , loss=  9.300296783447266\n",
            "Epoch number :  11\n",
            "batch_number=  0  , loss=  6.729569911956787\n",
            "batch_number=  50  , loss=  6.359178066253662\n",
            "batch_number=  100  , loss=  6.328660488128662\n",
            "batch_number=  150  , loss=  6.060091495513916\n",
            "batch_number=  200  , loss=  5.870344638824463\n",
            "batch_number=  250  , loss=  5.727363586425781\n",
            "batch_number=  300  , loss=  6.343437671661377\n",
            "batch_number=  350  , loss=  8.517660140991211\n",
            "batch_number=  400  , loss=  4.981924057006836\n",
            "batch_number=  450  , loss=  6.789545059204102\n",
            "batch_number=  500  , loss=  7.47053337097168\n",
            "Epoch number :  12\n",
            "batch_number=  0  , loss=  4.677030563354492\n",
            "batch_number=  50  , loss=  5.292934417724609\n",
            "batch_number=  100  , loss=  5.227730751037598\n",
            "batch_number=  150  , loss=  5.941134929656982\n",
            "batch_number=  200  , loss=  5.235023498535156\n",
            "batch_number=  250  , loss=  5.043570518493652\n",
            "batch_number=  300  , loss=  4.626490116119385\n",
            "batch_number=  350  , loss=  6.097892761230469\n",
            "batch_number=  400  , loss=  5.632411479949951\n",
            "batch_number=  450  , loss=  5.582882881164551\n",
            "batch_number=  500  , loss=  5.074521541595459\n",
            "Epoch number :  13\n",
            "batch_number=  0  , loss=  5.175504207611084\n",
            "batch_number=  50  , loss=  4.327646255493164\n",
            "batch_number=  100  , loss=  4.264767646789551\n",
            "batch_number=  150  , loss=  4.916294097900391\n",
            "batch_number=  200  , loss=  4.334856986999512\n",
            "batch_number=  250  , loss=  4.366790294647217\n",
            "batch_number=  300  , loss=  4.228521823883057\n",
            "batch_number=  350  , loss=  5.108013153076172\n",
            "batch_number=  400  , loss=  5.4376749992370605\n",
            "batch_number=  450  , loss=  5.239223480224609\n",
            "batch_number=  500  , loss=  4.621241569519043\n",
            "Epoch number :  14\n",
            "batch_number=  0  , loss=  4.649239540100098\n",
            "batch_number=  50  , loss=  3.811110496520996\n",
            "batch_number=  100  , loss=  3.808220863342285\n",
            "batch_number=  150  , loss=  4.4196672439575195\n",
            "batch_number=  200  , loss=  4.97533655166626\n",
            "batch_number=  250  , loss=  4.7248125076293945\n",
            "batch_number=  300  , loss=  4.228864669799805\n",
            "batch_number=  350  , loss=  3.582812786102295\n",
            "batch_number=  400  , loss=  4.470455646514893\n",
            "batch_number=  450  , loss=  4.432726860046387\n",
            "batch_number=  500  , loss=  5.070928573608398\n",
            "Epoch number :  15\n",
            "batch_number=  0  , loss=  3.011030435562134\n",
            "batch_number=  50  , loss=  4.311896800994873\n",
            "batch_number=  100  , loss=  3.385773181915283\n",
            "batch_number=  150  , loss=  4.011684417724609\n",
            "batch_number=  200  , loss=  3.6663737297058105\n",
            "batch_number=  250  , loss=  3.3584742546081543\n",
            "batch_number=  300  , loss=  4.045196056365967\n",
            "batch_number=  350  , loss=  4.021789073944092\n",
            "batch_number=  400  , loss=  3.909837245941162\n",
            "batch_number=  450  , loss=  4.0599188804626465\n",
            "batch_number=  500  , loss=  4.006832122802734\n",
            "Epoch number :  16\n",
            "batch_number=  0  , loss=  3.180819272994995\n",
            "batch_number=  50  , loss=  2.9537503719329834\n",
            "batch_number=  100  , loss=  3.322441577911377\n",
            "batch_number=  150  , loss=  2.8158388137817383\n",
            "batch_number=  200  , loss=  2.9026272296905518\n",
            "batch_number=  250  , loss=  2.6709117889404297\n",
            "batch_number=  300  , loss=  3.4273476600646973\n",
            "batch_number=  350  , loss=  3.4298298358917236\n",
            "batch_number=  400  , loss=  2.832690715789795\n",
            "batch_number=  450  , loss=  3.038553476333618\n",
            "batch_number=  500  , loss=  3.5179409980773926\n",
            "Epoch number :  17\n",
            "batch_number=  0  , loss=  2.7058756351470947\n",
            "batch_number=  50  , loss=  2.43204402923584\n",
            "batch_number=  100  , loss=  3.1885364055633545\n",
            "batch_number=  150  , loss=  2.545987367630005\n",
            "batch_number=  200  , loss=  2.9482359886169434\n",
            "batch_number=  250  , loss=  3.307699680328369\n",
            "batch_number=  300  , loss=  2.9809823036193848\n",
            "batch_number=  350  , loss=  2.734008312225342\n",
            "batch_number=  400  , loss=  2.350456714630127\n",
            "batch_number=  450  , loss=  5.077191352844238\n",
            "batch_number=  500  , loss=  3.776763677597046\n",
            "Epoch number :  18\n",
            "batch_number=  0  , loss=  2.9328269958496094\n",
            "batch_number=  50  , loss=  2.9008092880249023\n",
            "batch_number=  100  , loss=  2.731839418411255\n",
            "batch_number=  150  , loss=  2.9142966270446777\n",
            "batch_number=  200  , loss=  3.627150297164917\n",
            "batch_number=  250  , loss=  2.470444917678833\n",
            "batch_number=  300  , loss=  2.748420238494873\n",
            "batch_number=  350  , loss=  2.344797372817993\n",
            "batch_number=  400  , loss=  2.471072196960449\n",
            "batch_number=  450  , loss=  2.051539421081543\n",
            "batch_number=  500  , loss=  3.279442548751831\n",
            "Epoch number :  19\n",
            "batch_number=  0  , loss=  3.345405101776123\n",
            "batch_number=  50  , loss=  2.647348642349243\n",
            "batch_number=  100  , loss=  1.8872541189193726\n",
            "batch_number=  150  , loss=  2.3783035278320312\n",
            "batch_number=  200  , loss=  2.2412967681884766\n",
            "batch_number=  250  , loss=  2.2628514766693115\n",
            "batch_number=  300  , loss=  1.9929544925689697\n",
            "batch_number=  350  , loss=  1.8409298658370972\n",
            "batch_number=  400  , loss=  1.9289324283599854\n",
            "batch_number=  450  , loss=  2.0410735607147217\n",
            "batch_number=  500  , loss=  2.9463603496551514\n",
            "Epoch number :  20\n",
            "batch_number=  0  , loss=  2.159100294113159\n",
            "batch_number=  50  , loss=  1.844828486442566\n",
            "batch_number=  100  , loss=  2.0231125354766846\n",
            "batch_number=  150  , loss=  1.5834996700286865\n",
            "batch_number=  200  , loss=  1.704679250717163\n",
            "batch_number=  250  , loss=  1.4039106369018555\n",
            "batch_number=  300  , loss=  1.7667434215545654\n",
            "batch_number=  350  , loss=  2.2865052223205566\n",
            "batch_number=  400  , loss=  1.8307358026504517\n",
            "batch_number=  450  , loss=  1.8154250383377075\n",
            "batch_number=  500  , loss=  1.9126819372177124\n",
            "Epoch number :  21\n",
            "batch_number=  0  , loss=  1.3901817798614502\n",
            "batch_number=  50  , loss=  1.479201316833496\n",
            "batch_number=  100  , loss=  1.5008660554885864\n",
            "batch_number=  150  , loss=  1.5704989433288574\n",
            "batch_number=  200  , loss=  1.6521961688995361\n",
            "batch_number=  250  , loss=  1.585107684135437\n",
            "batch_number=  300  , loss=  1.5666216611862183\n",
            "batch_number=  350  , loss=  1.8274251222610474\n",
            "batch_number=  400  , loss=  1.7135436534881592\n",
            "batch_number=  450  , loss=  1.2950737476348877\n",
            "batch_number=  500  , loss=  7.248215675354004\n",
            "Epoch number :  22\n",
            "batch_number=  0  , loss=  2.2060959339141846\n",
            "batch_number=  50  , loss=  2.153703212738037\n",
            "batch_number=  100  , loss=  1.3074023723602295\n",
            "batch_number=  150  , loss=  1.2397183179855347\n",
            "batch_number=  200  , loss=  1.8548014163970947\n",
            "batch_number=  250  , loss=  1.5080057382583618\n",
            "batch_number=  300  , loss=  1.2509156465530396\n",
            "batch_number=  350  , loss=  1.1589360237121582\n",
            "batch_number=  400  , loss=  1.6165580749511719\n",
            "batch_number=  450  , loss=  1.7168927192687988\n",
            "batch_number=  500  , loss=  1.5641881227493286\n",
            "Epoch number :  23\n",
            "batch_number=  0  , loss=  1.013932228088379\n",
            "batch_number=  50  , loss=  1.1839827299118042\n",
            "batch_number=  100  , loss=  1.140909194946289\n",
            "batch_number=  150  , loss=  1.1591333150863647\n",
            "batch_number=  200  , loss=  1.0644481182098389\n",
            "batch_number=  250  , loss=  1.167999029159546\n",
            "batch_number=  300  , loss=  1.1019341945648193\n",
            "batch_number=  350  , loss=  1.1657660007476807\n",
            "batch_number=  400  , loss=  1.2470015287399292\n",
            "batch_number=  450  , loss=  1.3377742767333984\n",
            "batch_number=  500  , loss=  1.0628591775894165\n",
            "Epoch number :  24\n",
            "batch_number=  0  , loss=  0.999553918838501\n",
            "batch_number=  50  , loss=  1.1538801193237305\n",
            "batch_number=  100  , loss=  1.0212477445602417\n",
            "batch_number=  150  , loss=  0.9850158095359802\n",
            "batch_number=  200  , loss=  0.7822587490081787\n",
            "batch_number=  250  , loss=  0.897615373134613\n",
            "batch_number=  300  , loss=  0.8958598375320435\n",
            "batch_number=  350  , loss=  0.9866431951522827\n",
            "batch_number=  400  , loss=  1.7606335878372192\n",
            "batch_number=  450  , loss=  1.0430461168289185\n",
            "batch_number=  500  , loss=  1.1000875234603882\n",
            "Epoch number :  25\n",
            "batch_number=  0  , loss=  1.0309635400772095\n",
            "batch_number=  50  , loss=  0.7876712083816528\n",
            "batch_number=  100  , loss=  0.791533887386322\n",
            "batch_number=  150  , loss=  0.9388996958732605\n",
            "batch_number=  200  , loss=  0.9240601658821106\n",
            "batch_number=  250  , loss=  1.0088382959365845\n",
            "batch_number=  300  , loss=  0.9313915967941284\n",
            "batch_number=  350  , loss=  0.8774843215942383\n",
            "batch_number=  400  , loss=  0.9780108332633972\n",
            "batch_number=  450  , loss=  1.0762996673583984\n",
            "batch_number=  500  , loss=  0.7871901988983154\n",
            "Epoch number :  26\n",
            "batch_number=  0  , loss=  0.7797743678092957\n",
            "batch_number=  50  , loss=  0.7621593475341797\n",
            "batch_number=  100  , loss=  1.4343678951263428\n",
            "batch_number=  150  , loss=  1.1377100944519043\n",
            "batch_number=  200  , loss=  0.8507364988327026\n",
            "batch_number=  250  , loss=  0.8581410646438599\n",
            "batch_number=  300  , loss=  0.8703458309173584\n",
            "batch_number=  350  , loss=  1.3569518327713013\n",
            "batch_number=  400  , loss=  0.9218233227729797\n",
            "batch_number=  450  , loss=  0.9011809229850769\n",
            "batch_number=  500  , loss=  0.9687918424606323\n",
            "Epoch number :  27\n",
            "batch_number=  0  , loss=  0.8209639191627502\n",
            "batch_number=  50  , loss=  0.6414708495140076\n",
            "batch_number=  100  , loss=  0.7682798504829407\n",
            "batch_number=  150  , loss=  0.74637371301651\n",
            "batch_number=  200  , loss=  0.6613245010375977\n",
            "batch_number=  250  , loss=  0.8651604056358337\n",
            "batch_number=  300  , loss=  0.7193190455436707\n",
            "batch_number=  350  , loss=  1.6886677742004395\n",
            "batch_number=  400  , loss=  1.145134687423706\n",
            "batch_number=  450  , loss=  1.0057107210159302\n",
            "batch_number=  500  , loss=  0.7874621152877808\n",
            "Epoch number :  28\n",
            "batch_number=  0  , loss=  0.8811919093132019\n",
            "batch_number=  50  , loss=  1.2596403360366821\n",
            "batch_number=  100  , loss=  0.8989829421043396\n",
            "batch_number=  150  , loss=  0.7218083739280701\n",
            "batch_number=  200  , loss=  0.7381047606468201\n",
            "batch_number=  250  , loss=  0.641767680644989\n",
            "batch_number=  300  , loss=  0.6889217495918274\n",
            "batch_number=  350  , loss=  0.7462323904037476\n",
            "batch_number=  400  , loss=  0.7858322858810425\n",
            "batch_number=  450  , loss=  0.7354182004928589\n",
            "batch_number=  500  , loss=  0.6555162072181702\n",
            "Epoch number :  29\n",
            "batch_number=  0  , loss=  0.6322305202484131\n",
            "batch_number=  50  , loss=  0.5089738368988037\n",
            "batch_number=  100  , loss=  0.5808604955673218\n",
            "batch_number=  150  , loss=  0.5482310652732849\n",
            "batch_number=  200  , loss=  0.7362861633300781\n",
            "batch_number=  250  , loss=  0.669890820980072\n",
            "batch_number=  300  , loss=  0.5576699376106262\n",
            "batch_number=  350  , loss=  0.514519214630127\n",
            "batch_number=  400  , loss=  0.5630992650985718\n",
            "batch_number=  450  , loss=  0.6979377269744873\n",
            "batch_number=  500  , loss=  0.6001022458076477\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcchN2twVI9F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###Result text file generation cell \n",
        "\n",
        "validation_model = Model()\n",
        "validation_model.load_model(                    \"/content/drive/My Drive/Deep_project/version10_models\",\"epoch_29\")\n",
        "validation_model.eval_mode()\n",
        "validation_model.make_formula_file(train_x,     \"/content/drive/My Drive/Deep_project/version10_models/train_pred_v10_29.txt\")\n",
        "\n",
        "\n",
        "# python3 Evaluation/bleu_score.py --target-formulas Dataset/formulas/validation_formulas.txt --predicted-formulas danial_validation_v3_24.txt --ngram 5\n",
        "# python3 Evaluation/edit_distance.py --target-formulas Dataset/formulas/validation_formulas.txt --predicted-formulas danial_validation_v3_25.txt\n",
        "\n",
        "    \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iliqgvEX4MGM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###Chelknevis Cell\n",
        "\n",
        "import pickle\n",
        "f = open(\"/content/drive/My Drive/Deep_project/version10_models/optimizer_29\", \"wb\")\n",
        "\n",
        "pickle.dump(my_model.optimizer, f)\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}